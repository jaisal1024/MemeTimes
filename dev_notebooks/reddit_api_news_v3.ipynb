{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install praw\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import requests.auth\n",
    "# import pandas as pd\n",
    "# from newspaper import Article \n",
    "# import praw\n",
    "# from praw.models import MoreComments\n",
    "# import json\n",
    "# import pyimgflip\n",
    "# from sqlalchemy import create_engine\n",
    "# from sqlalchemy.types import VARCHAR\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "# import nltk\n",
    "# import re\n",
    "# from datetime import timedelta, datetime, date\n",
    "# import time\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as f:\n",
    "    data = json.load(f)\n",
    "reddit_cred = data['Reddit']\n",
    "watson_cred = data['Watson']\n",
    "newspaper_cred = data['News']\n",
    "sql_cred = data[\"SQL\"]\n",
    "img_cred = data[\"img\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': '_r9vZ43gHAboPQ',\n",
       " 'client_secret': 'ryqXnZEMztMEBwTGXT1oSJHSIqA',\n",
       " 'password': 'aFTeCTugiBEr',\n",
       " 'user_agent': 'meme-times/0.1 by jaisal1024',\n",
       " 'username': 'jaisal1024'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit = praw.Reddit(**reddit_cred)\n",
    "reddit_cred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meme_py = pyimgflip.Imgflip(username=img_cred[\"username\"], password=img_cred[\"password\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn_string = 'mysql://{user}:{password}@{host}/{db}?charset=utf8mb4'.format(\n",
    "    host = sql_cred[\"host\"], \n",
    "    user = sql_cred[\"user\"],\n",
    "    password = sql_cred[\"password\"], \n",
    "    db = 'MemeNews')\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_reddit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meme_ = generateMeme(2, True)\n",
    "meme_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0 0\n",
      "1 1\n",
      "commentless\n",
      "1 2\n",
      "2 3\n",
      "3 4\n",
      "commentless\n",
      "3 5\n",
      "4 6\n",
      "commentless\n",
      "4 7\n",
      "5 8\n",
      "commentless\n",
      "5 9\n"
     ]
    }
   ],
   "source": [
    "yest = (datetime.utcnow()-timedelta(hours = 24)).timestamp()\n",
    "num = 5\n",
    "query = '''SELECT * FROM MemeNews.Daily_Articles WHERE created > {0} ORDER BY score DESC LIMIT {1}'''.format(yest, 2*num)\n",
    "df_articles = pd.read_sql(query, engine)\n",
    "df = pd.DataFrame(columns = [\"post_id\", \"meme_url\", \"sentiment\"])\n",
    "j = 0\n",
    "k = 0\n",
    "print(df_articles.shape[0])\n",
    "while(j < num+1 and k < df_articles.shape[0]):\n",
    "    print(j,k)\n",
    "    id_ = df_articles[\"id\"][k]\n",
    "    k+=1\n",
    "    query = '''SELECT * FROM MemeNews.every_comment WHERE post_id LIKE '{0}' LIMIT 10'''.format(id_)\n",
    "    df_comments =  pd.read_sql(query, engine)\n",
    "    if (len(df_comments) < 3): \n",
    "        print(\"commentless\")\n",
    "        continue \n",
    "    max_emotion_final = [('Joy', -1, df_comments[\"body\"][0]),('Disgust', -1, df_comments[\"body\"][0])]\n",
    "    for body in df_comments[\"body\"]:\n",
    "        if (len(body) > 200):\n",
    "            continue\n",
    "        resp = extractEntitiesFromComment(body, watson_cred)\n",
    "        max_emotion = ('emotion', -1, '', 0)\n",
    "        for r in resp:\n",
    "            for key, value in r['emotion'].items():\n",
    "                if value > max_emotion[1]:\n",
    "                    max_emotion = (key, value, body, r['sentiment'])\n",
    "        if max_emotion[3] > 0: #sentiment is positive\n",
    "            if (max_emotion[1] > max_emotion_final[0][1]):\n",
    "                max_emotion_final[0] = max_emotion\n",
    "        elif max_emotion[1] > max_emotion_final[1][1]: #sentiment is negative\n",
    "            max_emotion_final[1] = max_emotion\n",
    "    urls = []\n",
    "    senti = []\n",
    "    if (max_emotion_final[0] == max_emotion_final[1]): #same emotion\n",
    "        query = '''SELECT * FROM MemeNews.Meme_Photos where emotion LIKE '{0}' ORDER BY RAND() LIMIT 2'''.format(max_emotion_1[0])\n",
    "        df_memes_photos = pd.read_sql(query, engine)\n",
    "        i = 0\n",
    "        for meme_photo in df_memes_photos:\n",
    "            result = meme_py.caption_image(meme_photo, max_emotion_final[i][2], \"\")\n",
    "            urls.append(result[\"url\"])\n",
    "            if i == 0:\n",
    "                senti.append('negative')\n",
    "            else:\n",
    "                senti.append('positive')\n",
    "            i+=1\n",
    "    else:\n",
    "        i = 0\n",
    "        for max_emotion in max_emotion_final:\n",
    "            query = '''SELECT * FROM MemeNews.Meme_Photos where emotion LIKE '{0}' ORDER BY RAND() LIMIT 1'''.format(max_emotion_final[i][0])\n",
    "            df_memes_photos = pd.read_sql(query, engine)\n",
    "            result = meme_py.caption_image(df_memes_photos[\"id\"], max_emotion_final[i][2], \"\")\n",
    "            urls.append(result[\"url\"])\n",
    "            if i == 0:\n",
    "                senti.append('negative')\n",
    "            else:\n",
    "                senti.append('positive')\n",
    "            i+=1\n",
    "\n",
    "    meme_dict = {\"post_id\": id_, \"meme_url\": urls, \"sentiment\": senti}\n",
    "    temp = pd.DataFrame(meme_dict, index = [2*j, 2*j+1])\n",
    "    df = df.append(temp)\n",
    "    j+=1 \n",
    "    \n",
    "df.to_sql('Memes', con = engine, if_exists='replace', dtype={'None':VARCHAR(5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meme_url</th>\n",
       "      <th>post_id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://i.imgflip.com/2or4ce.jpg</td>\n",
       "      <td>a52crr</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://i.imgflip.com/2or4cf.jpg</td>\n",
       "      <td>a52crr</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://i.imgflip.com/2or4cl.jpg</td>\n",
       "      <td>a56i3c</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://i.imgflip.com/2or4co.jpg</td>\n",
       "      <td>a56i3c</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://i.imgflip.com/2or4cv.jpg</td>\n",
       "      <td>a55k6m</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://i.imgflip.com/2or4cw.jpg</td>\n",
       "      <td>a55k6m</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://i.imgflip.com/2or4d3.jpg</td>\n",
       "      <td>a56i3c</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://i.imgflip.com/2or4d4.jpg</td>\n",
       "      <td>a56i3c</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://i.imgflip.com/2or4db.jpg</td>\n",
       "      <td>a564or</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://i.imgflip.com/2or4dd.jpg</td>\n",
       "      <td>a564or</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://i.imgflip.com/2or4do.jpg</td>\n",
       "      <td>a52t4n</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://i.imgflip.com/2or4dp.jpg</td>\n",
       "      <td>a52t4n</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           meme_url post_id sentiment\n",
       "0   http://i.imgflip.com/2or4ce.jpg  a52crr  negative\n",
       "1   http://i.imgflip.com/2or4cf.jpg  a52crr  positive\n",
       "2   http://i.imgflip.com/2or4cl.jpg  a56i3c  negative\n",
       "3   http://i.imgflip.com/2or4co.jpg  a56i3c  positive\n",
       "4   http://i.imgflip.com/2or4cv.jpg  a55k6m  negative\n",
       "5   http://i.imgflip.com/2or4cw.jpg  a55k6m  positive\n",
       "6   http://i.imgflip.com/2or4d3.jpg  a56i3c  negative\n",
       "7   http://i.imgflip.com/2or4d4.jpg  a56i3c  positive\n",
       "8   http://i.imgflip.com/2or4db.jpg  a564or  negative\n",
       "9   http://i.imgflip.com/2or4dd.jpg  a564or  positive\n",
       "10  http://i.imgflip.com/2or4do.jpg  a52t4n  negative\n",
       "11  http://i.imgflip.com/2or4dp.jpg  a52t4n  positive"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>meme_url</th>\n",
       "      <th>post_id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://i.imgflip.com/2or4k8.jpg</td>\n",
       "      <td>a52crr</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://i.imgflip.com/2or4kc.jpg</td>\n",
       "      <td>a52crr</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://i.imgflip.com/2or4kk.jpg</td>\n",
       "      <td>a56i3c</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://i.imgflip.com/2or4kl.jpg</td>\n",
       "      <td>a56i3c</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://i.imgflip.com/2or4ks.jpg</td>\n",
       "      <td>a55k6m</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>http://i.imgflip.com/2or4ku.jpg</td>\n",
       "      <td>a55k6m</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>http://i.imgflip.com/2or4l5.jpg</td>\n",
       "      <td>a56i3c</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>http://i.imgflip.com/2or4l9.jpg</td>\n",
       "      <td>a56i3c</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>http://i.imgflip.com/2or4lk.jpg</td>\n",
       "      <td>a564or</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>http://i.imgflip.com/2or4ll.jpg</td>\n",
       "      <td>a564or</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>http://i.imgflip.com/2or4lq.jpg</td>\n",
       "      <td>a52t4n</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>http://i.imgflip.com/2or4lr.jpg</td>\n",
       "      <td>a52t4n</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                         meme_url post_id sentiment\n",
       "0       0  http://i.imgflip.com/2or4k8.jpg  a52crr  negative\n",
       "1       1  http://i.imgflip.com/2or4kc.jpg  a52crr  positive\n",
       "2       2  http://i.imgflip.com/2or4kk.jpg  a56i3c  negative\n",
       "3       3  http://i.imgflip.com/2or4kl.jpg  a56i3c  positive\n",
       "4       4  http://i.imgflip.com/2or4ks.jpg  a55k6m  negative\n",
       "5       5  http://i.imgflip.com/2or4ku.jpg  a55k6m  positive\n",
       "6       6  http://i.imgflip.com/2or4l5.jpg  a56i3c  negative\n",
       "7       7  http://i.imgflip.com/2or4l9.jpg  a56i3c  positive\n",
       "8       8  http://i.imgflip.com/2or4lk.jpg  a564or  negative\n",
       "9       9  http://i.imgflip.com/2or4ll.jpg  a564or  positive\n",
       "10     10  http://i.imgflip.com/2or4lq.jpg  a52t4n  negative\n",
       "11     11  http://i.imgflip.com/2or4lr.jpg  a52t4n  positive"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''SELECT * FROM MemeNews.Memes'''\n",
    "df_memes_ = pd.read_sql(query, engine)\n",
    "df_memes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = '''SELECT * FROM MemeNews.Memes'''\n",
    "df_memes_ = pd.read_sql(query, engine)\n",
    "df_dict = {}\n",
    "for index, row in df_memes_.iterrows():\n",
    "    if (index %2 ==0): \n",
    "        query = '''SELECT * FROM MemeNews.Daily_Articles WHERE id LIKE '{0}' LIMIT 1'''.format(row['post_id'])\n",
    "        df_article = pd.read_sql(query, engine)\n",
    "        df_dict = df_article.iloc[0].to_dict()\n",
    "        print(df_dict['title'], df_dict['url'], df_dict['image'], df_dict['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def generateMeme(num, raw): \n",
    "    query = '''SELECT * FROM MemeNews.Daily_Articles ORDER BY score DESC LIMIT {0}'''.format(num)\n",
    "    df_articles = pd.read_sql(query, engine)\n",
    "    df = pd.DataFrame(columns = [\"post_id\", \"meme_url\", \"sentiment\"])\n",
    "    for j in range(num): \n",
    "        id_ = df_articles[\"id\"][j]\n",
    "        query = '''SELECT * FROM MemeNews.every_comment WHERE post_id LIKE '{0}' LIMIT 10'''.format(id_)\n",
    "        df_comments =  pd.read_sql(query, engine)\n",
    "        max_emotion_final = [('Joy', -1, df_comments[\"body\"][0]),('Disgust', -1, df_comments[\"body\"][0])]\n",
    "        for body in df_comments[\"body\"]: \n",
    "            if (len(body) > 200): \n",
    "                continue \n",
    "            resp = extractEntitiesFromComment(body)\n",
    "            max_emotion = ('emotion', -1, '', 0)\n",
    "            for r in resp: \n",
    "                for key, value in r['emotion'].items():\n",
    "                    print(key, value)\n",
    "                    if value > max_emotion[1]: \n",
    "                        max_emotion = (key, value, body, r['sentiment'])         \n",
    "            if max_emotion[3] > 0: #sentiment is positive\n",
    "                if (max_emotion[1] > max_emotion_final[0][1]):\n",
    "                    max_emotion_final[0] = max_emotion\n",
    "            elif max_emotion[1] > max_emotion_final[1][1]: #sentiment is negative\n",
    "                max_emotion_final[1] = max_emotion\n",
    "        urls = []\n",
    "        senti = []\n",
    "        if (max_emotion_final[0] == max_emotion_final[1]): #same emotion   \n",
    "            query = '''SELECT * FROM MemeNews.Meme_Photos where emotion LIKE '{0}' ORDER BY RAND() LIMIT 2'''.format(max_emotion_1[0])\n",
    "            df_memes_photos = pd.read_sql(query, engine)\n",
    "            i = 0\n",
    "            for meme_photo in df_memes_photos: \n",
    "                result = meme_py.caption_image(meme_photo, max_emotion_final[i][2], \"\")\n",
    "                urls.append(result[\"url\"])\n",
    "                if i == 0: \n",
    "                    senti.append('negative')\n",
    "                else: \n",
    "                    senti.append('positive')\n",
    "                i+=1\n",
    "        else: \n",
    "            i = 0\n",
    "            for max_emotion in max_emotion_final: \n",
    "                query = '''SELECT * FROM MemeNews.Meme_Photos where emotion LIKE '{0}' ORDER BY RAND() LIMIT 1'''.format(max_emotion_final[i][0])\n",
    "                df_memes_photos = pd.read_sql(query, engine)\n",
    "                result = meme_py.caption_image(df_memes_photos[\"id\"], max_emotion_final[i][2], \"\")\n",
    "                urls.append(result[\"url\"])\n",
    "                if i == 0: \n",
    "                    senti.append('negative')\n",
    "                else: \n",
    "                    senti.append('positive')\n",
    "                i+=1\n",
    "                \n",
    "        meme_dict = {\"post_id\": id_, \"meme_url\": urls, \"sentiment\": senti}\n",
    "        temp = pd.DataFrame(meme_dict, index = [2*j, 2*j+1])\n",
    "        df = df.append(temp)\n",
    "    status = df.to_sql('Memes_Test', con = engine, if_exists='replace', dtype={'None':VARCHAR(5)})\n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def scrape_reddit(limit_):  \n",
    "    yest = (datetime.utcnow()-timedelta(hours = 12)).timestamp()\n",
    "    i = 0\n",
    "    for submission in reddit.subreddit('news').hot(limit=limit_):\n",
    "        #submission.created > yest\n",
    "        if (True):\n",
    "            query_comments = '''SELECT * FROM MemeNews.every_test_comment  WHERE post_id LIKE '{0}' LIMIT 1 '''.format(submission.id)   \n",
    "            query_articles = '''SELECT * FROM MemeNews.Daily_Test_Articles  WHERE id LIKE '{0}' LIMIT 1 '''.format(submission.id) \n",
    "            engine.execute(query_articles)\n",
    "            if ():  \n",
    "                continue \n",
    "            submission.comment_sort = 'best'\n",
    "            article = Article(submission.url)\n",
    "            try:\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article.nlp()\n",
    "                article.fetch_images()\n",
    "            except:\n",
    "                continue\n",
    "            articles_dict = { \n",
    "                \"title\": re.sub(r'[^\\x00-\\x7F]', '', submission.title.replace('\"', \"'\")), \n",
    "                \"score\": submission.score, \n",
    "                \"id\": submission.id, \n",
    "                \"url\": submission.url, \n",
    "                \"comms_num\": submission.num_comments, \n",
    "                \"created\": submission.created, \n",
    "                \"body\": re.sub(r'[^\\x00-\\x7F]', '',article.text.replace('\"', \"'\")),\n",
    "                \"image\": article.top_image,\n",
    "                \"keywords\":', '.join(article.keywords).replace('\"', \"'\"),\n",
    "                \"summary\": re.sub(r'[^\\x00-\\x7F]', '', article.summary.replace('\"', \"'\"))\n",
    "            }\n",
    "            #add articles\n",
    "            articles_data = pd.DataFrame(articles_dict, index = [i])\n",
    "            articles_data.to_sql('Daily_Test_Articles', con = engine, if_exists='append', dtype={'None':VARCHAR(5)})\n",
    "            print(\"article added\")\n",
    "            if (engine.execute(query_comments)): \n",
    "                continue\n",
    "            comment_dict = { \n",
    "                \"post_id\":[], \n",
    "                'post_title':[],\n",
    "                \"id\": [],\n",
    "                \"author\":[], \n",
    "                \"body\":[],\n",
    "                \"created\": [],\n",
    "                'score':[],\n",
    "                'is_submitter':[],\n",
    "                'parent_id':[]}\n",
    "            for top_level_comment in submission.comments.list()[:100]:\n",
    "                try: \n",
    "                    comment_dict['is_submitter'].append(top_level_comment.is_submitter)\n",
    "                    comment_dict['post_id'].append(submission.id)\n",
    "                    comment_dict['id'].append(top_level_comment.id)\n",
    "                    comment_dict['author'].append(top_level_comment.author)\n",
    "                    comment_dict['body'].append(re.sub(r'[^\\x00-\\x7F]', '', top_level_comment.body))\n",
    "                    comment_dict['score'].append(top_level_comment.score)\n",
    "                    comment_dict['created'].append(top_level_comment.created_utc)\n",
    "                    comment_dict['parent_id'].append(top_level_comment.parent_id)\n",
    "                    comment_dict['post_title'].append(submission.title)\n",
    "                except: \n",
    "                    continue \n",
    "            comment_data = pd.DataFrame(comment_dict)\n",
    "            comment_data.to_sql('every_test_comment', con = engine, if_exists='append', dtype={'None':VARCHAR(5)})\n",
    "            print(\"comments added\")\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "articles_data.to_sql('Daily_Articles', con = engine, if_exists='append', dtype={'None':VARCHAR(5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "comment_data.to_sql('every_comment', con = engine, if_exists='append', dtype={'None':VARCHAR(5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comment_dict = { \"post_id\":[], \n",
    "                  'post_title':[],\n",
    "\n",
    "                \"id\": [],\n",
    "                        \"author\":[], \n",
    "                        \"body\":[],\n",
    "                        \"created\": [],\n",
    "                         'score':[],\n",
    "                 'is_submitter':[],\n",
    "                  'parent_id':[]}\n",
    "articles_dict = { \"title\":[], \n",
    "                \"score\":[], \n",
    "                \"id\":[], \"url\":[], \n",
    "                \"comms_num\": [], \n",
    "                \"created\": [], \n",
    "                \"body\":[],\n",
    "                \"image\":[],\n",
    "                \"keywords\":[],\n",
    "                \"summary\":[]\n",
    "                  }\n",
    "\n",
    "        articles_dict[\"title\"].append(re.sub(r'[^\\x00-\\x7F]', '', submission.title.replace('\"', \"'\")))\n",
    "        articles_dict[\"score\"].append(submission.score)\n",
    "        articles_dict[\"id\"].append(submission.id)\n",
    "        articles_dict[\"url\"].append(submission.url)\n",
    "        articles_dict[\"comms_num\"].append(submission.num_comments)\n",
    "        articles_dict[\"created\"].append(submission.created)\n",
    "        articles_dict[\"body\"].append(re.sub(r'[^\\x00-\\x7F]', '',article.text.replace('\"', \"'\")))\n",
    "        articles_dict[\"image\"].append(article.top_image)\n",
    "        articles_dict[\"keywords\"].append(', '.join(article.keywords).replace('\"', \"'\"))\n",
    "        articles_dict[\"summary\"].append(re.sub(r'[^\\x00-\\x7F]', '', article.summary.replace('\"', \"'\")))\n",
    "        articles_data = pd.DataFrame(articles_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def extractEntitiesFromUrl(url):\n",
    "    endpoint_watson = \"https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze\"\n",
    "    params = {\n",
    "        'version': '2017-02-27',\n",
    "    }\n",
    "    headers = { \n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    \n",
    "    watson_options = {\n",
    "      \"url\": url,\n",
    "      \"features\": {\n",
    "        \"entities\": {\n",
    "          \"sentiment\": True,\n",
    "          \"relevance\": True,   \n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    username = watson_cred[\"username\"]\n",
    "    password = watson_cred[\"password\"]\n",
    "\n",
    "    resp = requests.post(endpoint_watson, \n",
    "                         data=json.dumps(watson_options), \n",
    "                         headers=headers, \n",
    "                         params=params, \n",
    "                         auth=(username, password) \n",
    "                        )\n",
    "    data=resp.json()\n",
    "    # create and return a dictionary for each entity with entity name, url, source, relevance and sentiment score as keys\n",
    "    entities_list=[]\n",
    "#     print(data)\n",
    "    for entity in data[\"entities\"]:\n",
    "        entity_dict={}\n",
    "        entity_dict[\"entity\"]=entity[\"text\"]\n",
    "        entity_dict[\"relevance\"]=entity[\"relevance\"]\n",
    "        entity_dict[\"sentiment\"]=entity[\"sentiment\"][\"score\"]\n",
    "        entity_dict[\"entity\"]=entity[\"text\"]\n",
    "        entities_list.append(entity_dict)\n",
    "        \n",
    "    return entities_list\n",
    "\n",
    "def extractEntitiesFromComment(comment):\n",
    "    endpoint_watson = \"https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze\"\n",
    "    params = {\n",
    "        'version': '2017-02-27',\n",
    "    }\n",
    "    headers = { \n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    \n",
    "    watson_options = {\n",
    "      \"text\": comment,\n",
    "      \"features\": {\n",
    "        \"entities\": {\n",
    "          \"sentiment\": True,\n",
    "          \"relevance\": True,\n",
    "          \"emotion\" :True,\n",
    "            'categories': True,\n",
    "            'semantics': True\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    username = watson_cred[\"username\"]\n",
    "    password = watson_cred[\"password\"]\n",
    "\n",
    "    resp = requests.post(endpoint_watson, \n",
    "                         data=json.dumps(watson_options), \n",
    "                         headers=headers, \n",
    "                         params=params, \n",
    "                         auth=(username, password) \n",
    "                        )\n",
    "    data=resp.json()\n",
    "    # create and return a dictionary for each entity with entity name, url, source, relevance and sentiment score as keys\n",
    "    entities_list=[]\n",
    "    for entity in data[\"entities\"]:\n",
    "        entity_dict={}\n",
    "        try: \n",
    "            entity_dict[\"emotion\"]=entity[\"emotion\"]\n",
    "            entity_dict[\"entity\"]=entity[\"text\"]\n",
    "            entity_dict[\"relevance\"]=entity[\"relevance\"]\n",
    "            entity_dict[\"sentiment\"]=entity[\"sentiment\"][\"score\"]\n",
    "            entities_list.append(entity_dict)\n",
    "        except: \n",
    "            continue\n",
    "        \n",
    "    return entities_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
